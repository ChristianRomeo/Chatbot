{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data manipulation to consolidate the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Ray execution environment not yet initialized. Initializing...\n",
      "To remove this warning, run the following python code before doing dataframe operations:\n",
      "\n",
      "    import ray\n",
      "    ray.init()\n",
      "\n",
      "2023-12-23 15:56:26,356\tINFO worker.py:1673 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               label  \\\n",
      "0  Event name: Séjour : le Brame du Cerf à la Rés...   \n",
      "1                    Event name: MEDIATHEQUE D'AURON   \n",
      "2  Event name: ALL FRENCH RIVIERA FOR ONE DAY SIG...   \n",
      "3  Event name: Conférence \"Demain l'Humain\" - Dem...   \n",
      "4  Event name: Visite privée : excursion d'une de...   \n",
      "\n",
      "                                         description  \\\n",
      "0  Event description: Séjour VIP  Tout le cérémo...   \n",
      "1                                                      \n",
      "2  Event description: <p><strong>Your guide will ...   \n",
      "3  Event description: Lieu: <strong>Palais des Co...   \n",
      "4  Event description: Découvrez la richesse des s...   \n",
      "\n",
      "                                            category  \\\n",
      "0                  Event category: Nature et détente   \n",
      "1  Event category: Activités sportives, culturell...   \n",
      "2                    Event category: Circuits privés   \n",
      "3                                                      \n",
      "4                    Event category: Circuits privés   \n",
      "\n",
      "                    subject  \\\n",
      "0  Event subject: Excursion   \n",
      "1                             \n",
      "2                             \n",
      "3                             \n",
      "4                             \n",
      "\n",
      "                                          placeLabel  \\\n",
      "0  Event place: Séjour : le Brame du Cerf à la Ré...   \n",
      "1                   Event place: MEDIATHEQUE D'AURON   \n",
      "2                                Event place: Cannes   \n",
      "3                    Event place: Palais des Congrès   \n",
      "4                                  Event place: Nice   \n",
      "\n",
      "                                  placeLocality  \\\n",
      "0                   Event place location: Andon   \n",
      "1  Event place location: SAINT-ETIENNE-DE-TINEE   \n",
      "2                  Event place location: Cannes   \n",
      "3                 Event place location: Antibes   \n",
      "4                    Event place location: Nice   \n",
      "\n",
      "                                timeBegin  \\\n",
      "0  Event time begin: 2017-09-15T00:00:00Z   \n",
      "1  Event time begin: 2020-01-01T00:00:00Z   \n",
      "2  Event time begin: 2018-07-24T16:29:00Z   \n",
      "3  Event time begin: 2019-03-02T00:00:00Z   \n",
      "4  Event time begin: 2018-07-24T16:29:00Z   \n",
      "\n",
      "                                timeEnd  \n",
      "0  Event time end: 2017-10-30T00:00:00Z  \n",
      "1  Event time end: 2020-12-31T00:00:00Z  \n",
      "2  Event time end: 2020-07-24T16:29:00Z  \n",
      "3  Event time end: 2019-03-02T23:59:00Z  \n",
      "4  Event time end: 2020-07-24T16:29:00Z  \n",
      "                                               label description  \\\n",
      "0                          Place name: AZUR Batiment               \n",
      "1                           Place name: Happy Market               \n",
      "2  Place name: Stratege Diagnostics Immobiliers A...               \n",
      "3                   Place name: Classic Racer Cannes               \n",
      "4                         Place name: Photo for Love               \n",
      "\n",
      "                     businessTypeLabel                locality  \n",
      "0                                       Place location: Cannes  \n",
      "1  Place business type: Shop & Service  Place location: Cannes  \n",
      "2                                       Place location: Cannes  \n",
      "3                                       Place location: Cannes  \n",
      "4  Place business type: Shop & Service  Place location: Cannes  \n"
     ]
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dfe = pd.read_csv('./oldData/3cixty_cotedazur_events.csv', dtype=str)\n",
    "\n",
    "dfe['label'] = dfe['label'].apply(lambda x: 'Event name: ' + str(x) if pd.notna(x) else x)\n",
    "dfe['description'] = dfe['description'].apply(lambda x: 'Event description: ' + str(x) if pd.notna(x) else x)\n",
    "dfe['category'] = dfe['category'].apply(lambda x: 'Event category: ' + str(x) if pd.notna(x) else x)\n",
    "dfe['subject'] = dfe['subject'].apply(lambda x: 'Event subject: ' + str(x) if pd.notna(x) else x)\n",
    "dfe['placeLabel'] = dfe['placeLabel'].apply(lambda x: 'Event place: ' + str(x) if pd.notna(x) else x)\n",
    "dfe['placeLocality'] = dfe['placeLocality'].apply(lambda x: 'Event place location: ' + str(x) if pd.notna(x) else x)\n",
    "dfe['timeBegin'] = dfe['timeBegin'].apply(lambda x: 'Event time begin: ' + str(x) if pd.notna(x) else x)\n",
    "dfe['timeEnd'] = dfe['timeEnd'].apply(lambda x: 'Event time end: ' + str(x) if pd.notna(x) else x)\n",
    "\n",
    "dfe = dfe.groupby('event', as_index=False).agg({\n",
    "    'label': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'description': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'category': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'subject': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'placeLabel': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'placeLocality': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'timeBegin': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'timeEnd': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else ''\n",
    "})\n",
    "\n",
    "dfe.pop('event')\n",
    "\n",
    "print(dfe.head())\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "dfe.to_csv('./data/events.csv', index=False)\n",
    "\n",
    "dfp = pd.read_csv('./oldData/3cixty_cotedazur_places.csv', dtype=str)\n",
    "\n",
    "dfp['label'] = dfp['label'].apply(lambda x: 'Place name: ' + str(x) if pd.notna(x) else x)\n",
    "dfp['description'] = dfp['description'].apply(lambda x: 'Place description: ' + str(x) if pd.notna(x) else x)\n",
    "dfp['businessTypeLabel'] = dfp['businessTypeLabel'].apply(lambda x: 'Place business type: ' + str(x) if pd.notna(x) else x)\n",
    "dfp['locality'] = dfp['locality'].apply(lambda x: 'Place location: ' + str(x) if pd.notna(x) else x)\n",
    "\n",
    "dfp = dfp.groupby('place', as_index=False, sort=False).agg({\n",
    "    'label': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'description': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'businessTypeLabel': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else '',\n",
    "    'locality': lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else ''\n",
    "})\n",
    "\n",
    "dfp.pop('place')\n",
    "\n",
    "print(dfp.head())\n",
    "\n",
    "dfp.to_csv('./data/places.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from llama_index import VectorStoreIndex, ServiceContext, Document, StorageContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "import chromadb\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\chris/.cache\\torch\\sentence_transformers\\dangvantuan_sentence-camembert-large. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = (\n",
    "    \"You are a knowledgeable Tourism Assistant designed to provide visitors with \"\n",
    "    \"information, recommendations, and tips for exploring and enjoying their destination. \"\n",
    "    \"The assistant is familiar with a wide range of topics including historical sites, \"\n",
    "    \"cultural events, local cuisine, accommodations, transportation options, and hidden gems. \"\n",
    "    \"It offers up-to-date and personalized information to help tourists make the most of their trip.\"\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.6, system_prompt=system_prompt), \n",
    "                                               embed_model=LangchainEmbedding(HuggingFaceEmbeddings(model_name='dangvantuan/sentence-camembert-large', \n",
    "                                                                                                    model_kwargs = {'device': 'cuda:0'})))\n",
    "\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"tourism\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "storage_context = StorageContext.from_defaults(vector_store=ChromaVectorStore(chroma_collection=chroma_collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to compute the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3536.09it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [01:06<00:00, 82.65it/s] \n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3600.37it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 119.96it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3302.42it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.61it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 4167.63it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:46<00:00, 116.90it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3278.39it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [01:11<00:00, 76.81it/s] \n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 4178.96it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 119.50it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3067.13it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [01:08<00:00, 80.04it/s] \n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3294.81it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [01:30<00:00, 60.46it/s] \n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3320.93it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 119.18it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2777.18it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 121.26it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3575.81it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:46<00:00, 116.69it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 2591.43it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:46<00:00, 117.42it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2971.01it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [01:43<00:00, 52.72it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2960.01it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 119.59it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 2699.76it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 121.17it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2815.42it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [01:29<00:00, 61.09it/s] \n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 2164.53it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [01:06<00:00, 81.86it/s] \n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 2359.36it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:46<00:00, 117.77it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 2306.61it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 121.43it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3342.59it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [01:24<00:00, 64.91it/s] \n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 1925.90it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 119.96it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3001.45it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.40it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3760.96it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 121.21it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3421.50it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:49<00:00, 110.26it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:52<00:00, 104.29it/s] \n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 118.77it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2830.10it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 120.79it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 2653.99it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 118.79it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 2241.58it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:52<00:00, 103.30it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2991.62it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [01:10<00:00, 77.58it/s] \n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 2652.88it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 120.27it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3632.58it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.21it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2987.72it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 120.37it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3041.94it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 119.33it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3905.05it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 123.25it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2897.73it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.32it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:29<00:00, 188.17it/s] \n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:43<00:00, 124.12it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2806.76it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.34it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3063.59it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 120.69it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3496.98it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 123.34it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3417.76it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 123.46it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 4403.06it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 123.61it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 4365.13it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 123.23it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3897.42it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:43<00:00, 124.28it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3582.23it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.22it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3545.78it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 119.59it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3552.53it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 119.40it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3616.83it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 123.60it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 2736.03it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:46<00:00, 117.82it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3287.31it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 121.09it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3672.23it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.49it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3677.81it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 120.91it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3792.13it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 120.90it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3154.71it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.98it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 4090.35it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:48<00:00, 112.35it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3420.10it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:46<00:00, 116.79it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:02<00:00, 1959.97it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.32it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3126.73it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 122.74it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:58<00:00, 94.10it/s]  \n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:44<00:00, 121.68it/s]\n",
      "Parsing nodes: 100%|██████████| 5461/5461 [00:01<00:00, 3645.11it/s]\n",
      "Generating embeddings: 100%|██████████| 5461/5461 [00:45<00:00, 118.97it/s]\n",
      "Parsing nodes: 100%|██████████| 3890/3890 [00:01<00:00, 3834.19it/s]\n",
      "Generating embeddings: 100%|██████████| 3890/3890 [00:32<00:00, 120.39it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = glob.glob('./data/*.csv')\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file, dtype=str, parse_dates=True)\n",
    "\n",
    "    # Convert the DataFrame into a list of Document objects\n",
    "    docs = [Document(doc_id=str(i), text=row.to_string()) for i, row in df.iterrows()]\n",
    "\n",
    "    # Add the documents to the list\n",
    "    documents.extend(docs)\n",
    "\n",
    "batch_size = 5461  # Maximum batch size\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch = documents[i:i+batch_size]\n",
    "    # Now add the batch to the index\n",
    "    index = VectorStoreIndex.from_documents(batch, service_context=service_context, storage_context=storage_context, show_progress=True, use_asynch=True)\n",
    "    storage_context.persist(persist_dir=f\"./chroma_db\")\n",
    "#storage_context.persist(persist_dir=f\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the index is already there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(ChromaVectorStore(chroma_collection=chroma_collection), storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(index\u001b[38;5;241m.\u001b[39mas_query_engine()\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter your query: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:1251\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1249\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:1295\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "print(index.as_query_engine().query(input(\"Enter your query: \")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
